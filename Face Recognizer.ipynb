{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognizer using pre trained model\n",
    "\n",
    "Below is the implementation of a face recognizer which can identify the face of the person showing on a web cam. We will apply transfer learning and use pre-trained weights of VGG Face model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the requried libraries\n",
    "Here we are importing all the requried libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contructing the neural network model\n",
    "The model here constructed is based on VGG face model.It has 22 layers and 37 deep units.\n",
    "![Vgg face model](images/vgg-face-model.png)\n",
    "The structure of the VGG-Face model is demonstrated below.\n",
    "\n",
    "\n",
    "The VGG-Face architure can be to be understood clear with the help of following image,\n",
    "![Vgg face model](images/vgg-face-architecture.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model with pretrained weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "model.load_weights('C:\\\\Users\\\\Dell\\\\Desktop\\\\mlops\\\\Face-recognizer-using-pre-trained-model-master\\\\Face-recognizer-using-pre-trained-model-masterf/vgg_face_weights.h5')\n",
    "# model.load_weights('C:/Users/Abhay/Desktop/Face-recognition-using-deep-learning-master/vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>image_to_embedding</font> function        \n",
    "When the model is loaded with pre trained weights, then we can create the **2622 dimensional embedding vectors** for all the face images stored in the \"images\" folder. **\"image_to_embedding\"** function pass an image to the VGG network to generate the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_embedding(image, model):\n",
    "    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
    "    image = cv2.resize(image, (224, 224)) \n",
    "    img = image[...,::-1]\n",
    "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>recognize_face</font> function\n",
    "This function calculate similarity between the captured image and the images that are already been stored in the folder. It passes the image to the trained neural network to generate its embedding vector. Which is then compared with all the embedding vectors of the images stored by calculating L2 Euclidean distance and the cosine distance. \n",
    "\n",
    "Here I am using both the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) to see for the match.Greater the Cosine similarity between two images then there is greater probability of having the image of same person. Smaller the Euclidean distance between two images then there is greater probability of having the image of same person. \n",
    "\n",
    "Here I am setting the thresholds for the Euclidean distance and Cosine Similarity are 0.17 and 0.70 (which can be adjusted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_face(face_image, input_embeddings, model):   \n",
    "    \n",
    "    embedding = image_to_embedding(face_image, model)\n",
    "    \n",
    "    minimum_distance = 200\n",
    "    minimum_distance1 = 0\n",
    "    name = None\n",
    "    \n",
    "    # Loop over  names and encodings.\n",
    "    for (input_name, input_embedding) in input_embeddings.items():\n",
    "        \n",
    "       \n",
    "        euclidean_distance = np.linalg.norm(embedding-input_embedding) * 1000\n",
    "        \n",
    "        a = np.matmul(embedding,input_embedding.T)\n",
    "        b = np.sqrt(np.matmul(embedding,embedding.T))\n",
    "        c = np.sqrt(np.matmul(input_embedding,input_embedding.T))\n",
    "        cosine_dist = a / ( b * c )\n",
    "        \n",
    "        #print(dist1)\n",
    "        #print (cosine_dist)\n",
    "        \n",
    "      \n",
    "        print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
    "\n",
    "        \n",
    "        if euclidean_distance < minimum_distance and cosine_dist > minimum_distance1:\n",
    "            minimum_distance = euclidean_distance\n",
    "            minimum_distance1 = cosine_dist\n",
    "            name = input_name[0]\n",
    "            \n",
    "    \n",
    "    if minimum_distance < 0.17 and minimum_distance1 > 0.70:\n",
    "        return str(name)\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>create_input_image_embeddings</font> function\n",
    "This function generates 512 dimensional image ebeddings of all the images stored in the \"imagesv1\" containg folders with person  names with images.We feed forwarding the images to a trained neural network. It creates a dictionary with key as the name of the face and value as embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def create_input_image_embeddings():      \n",
    "    input_embeddings = {}\n",
    "\n",
    "    for file in glob.glob(\"imagesv1/*\"):\n",
    "        for file1 in glob.glob(file+\"/*\"):\n",
    "            person_name_image = os.path.splitext(os.path.basename(file1))[0]\n",
    "            person_name = os.path.splitext(os.path.basename(file))[0]\n",
    "#             print(person_name)\n",
    "            image_file = cv2.imread(file1, 1)\n",
    "            input_embeddings[person_name,person_name_image] = image_to_embedding(image_file, model)\n",
    "\n",
    "    return input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About <font color=blue>recognize_faces_in_cam</font> function\n",
    "This function capture image from the webcam, detect a face in it and crop the image to have a face only, which is then passed to recognize_face function. Here for the face detection we are using Haarcascade model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces_in_cam(input_embeddings):\n",
    "    \n",
    "\n",
    "    cv2.namedWindow(\"Face Recognizer\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "   \n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    \n",
    "    while vc.isOpened():\n",
    "        _, frame = vc.read()\n",
    "        img = frame\n",
    "        height, width, channels = frame.shape\n",
    "\n",
    "        \n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        # Loop through all the faces detected \n",
    "        identities = []\n",
    "        for (x, y, w, h) in faces:\n",
    "            x1 = x\n",
    "            y1 = y\n",
    "            x2 = x+w\n",
    "            y2 = y+h\n",
    "\n",
    "           \n",
    "            \n",
    "            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n",
    "            identity = recognize_face(face_image, input_embeddings, model)\n",
    "            \n",
    "            \n",
    "\n",
    "            if identity is not None:\n",
    "                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,255,255),2)\n",
    "                cv2.putText(img, str(identity), (x1+5,y1-5), font, 1, (255,255,255), 2)\n",
    "        \n",
    "        key = cv2.waitKey(100)\n",
    "        cv2.imshow(\"Face Recognizer\", img)\n",
    "\n",
    "        if key == 27: # exit on ESC\n",
    "            break\n",
    "    vc.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing the face image\n",
    "Following code captures 10 face images of the person. They all are stored in **\"imagesv1\"** folder.After every run this will create a new folder with name **\"new user_(current date and time)\"** (inorder to avoid duplication of the folder name).In this sub folder there will be 10 images of detected faces will be strored.You have to rename the folder name with the person name in order to get the correct name of person in output.You can also remove the images which are not much good from set of 10 images.\n",
    "\n",
    "These images will be used for recognizing the the identity of the person using one shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "#  #change - to get current time and date\n",
    "from datetime import datetime\n",
    "date_time = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "#  #change create a new folder to save images\n",
    "newpath =r'path'+date_time \n",
    "image_path = r'imagesv1\\newUser'+date_time\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "    \n",
    "    \n",
    "count = 0\n",
    "while(True):\n",
    "    ret, img = cam.read()\n",
    "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        x1 = x\n",
    "        y1 = y\n",
    "        x2 = x+w\n",
    "        y2 = y+h\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), (255,255,255), 2)     \n",
    "        count += 1\n",
    "        # Save the captured image into the datasets folder\n",
    "        cv2.imwrite(image_path+\"/User_\" + str(count) + \".jpg\", img[y1:y2,x1:x2])\n",
    "        cv2.imshow('image', img)\n",
    "    k = cv2.waitKey(200) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "    elif count >= 10: # Take 10 face sample and stop video\n",
    "         break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = create_input_image_embeddings()\n",
    "recognize_faces_in_cam(input_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
